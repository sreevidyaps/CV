{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap =cv2.VideoCapture(0) #0 is index of camera\n",
    "while True : #infinite loop for video capture\n",
    "    ret,frame= cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame from camera\")\n",
    "        break\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(video_path):\n",
    "    cap =cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: could not open video file\")\n",
    "        return\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        cv2.imshow(\"Video\",frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "video_path = (r\"C:\\Users\\AMZ PC\\OneDrive\\Desktop\\CV\\Smoke detection\\3826855-hd_1920_1080_24fps.mp4\")\n",
    "play_video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to load non_fire.189.png\n",
      "Total images loaded: 998\n",
      "Images shape: (998, 48, 48)\n",
      "Labels shape: (998,)\n",
      "Sample labels: [0 0 0 0 0 0 0 0 0 0]\n",
      "Type of images array: <class 'numpy.ndarray'>\n",
      "Type of labels array: <class 'numpy.ndarray'>\n",
      "First image shape: (48, 48)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "training_data=[r\"C:\\Users\\AMZ PC\\OneDrive\\Desktop\\CV\\Smoke detection\\fire_dataset\\fire_images\", \n",
    "               r\"C:\\Users\\AMZ PC\\OneDrive\\Desktop\\CV\\Smoke detection\\fire_dataset\\non_fire_images\"]\n",
    "\n",
    "def load_images(training_data):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(len(training_data)):\n",
    "        folder = training_data[i]\n",
    "        label = i  # 0 for fire, 1 for non-fire\n",
    "        for filename in os.listdir(folder):\n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    # Resize image to 48x48 pixels\n",
    "                    img = cv2.resize(img, (48, 48))\n",
    "                    images.append(img)\n",
    "                    labels.append(label)  # Correct label assignment\n",
    "                else:\n",
    "                    print(f\"Warning: Failed to load {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: loading image {os.path.join(folder, filename)}: {e}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Debugging print statements to check data\n",
    "    print(f\"Total images loaded: {len(images)}\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Sample labels: {labels[:10]}\")  # Print first 10 labels for debugging\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load images and labels\n",
    "images, labels = load_images(training_data)\n",
    "\n",
    "# Further Debugging\n",
    "print(f\"Type of images array: {type(images)}\")\n",
    "print(f\"Type of labels array: {type(labels)}\")\n",
    "print(f\"First image shape: {images[0].shape}\")  # Check shape of first image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "x_train, x_test , y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "x_train= x_train.reshape(x_train.shape[0],48,48,1).astype('float32')/255\n",
    "x_test= x_test.reshape(x_test.shape[0],48,48,1).astype('float32')/255\n",
    "\n",
    "y_train = to_categorical(y_train) \n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 269ms/step - accuracy: 0.6113 - loss: 0.7700 - val_accuracy: 0.7300 - val_loss: 0.5271\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 236ms/step - accuracy: 0.7637 - loss: 0.5339 - val_accuracy: 0.8250 - val_loss: 0.3916\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 241ms/step - accuracy: 0.8245 - loss: 0.3848 - val_accuracy: 0.8250 - val_loss: 0.3467\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 231ms/step - accuracy: 0.8343 - loss: 0.3695 - val_accuracy: 0.8450 - val_loss: 0.3164\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - accuracy: 0.8775 - loss: 0.2790 - val_accuracy: 0.8750 - val_loss: 0.3005\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - accuracy: 0.8671 - loss: 0.3040 - val_accuracy: 0.8850 - val_loss: 0.3062\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - accuracy: 0.8841 - loss: 0.2694 - val_accuracy: 0.8650 - val_loss: 0.3109\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 236ms/step - accuracy: 0.8845 - loss: 0.2658 - val_accuracy: 0.8800 - val_loss: 0.2922\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - accuracy: 0.9087 - loss: 0.2126 - val_accuracy: 0.8900 - val_loss: 0.2999\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 256ms/step - accuracy: 0.9067 - loss: 0.2085 - val_accuracy: 0.8800 - val_loss: 0.2915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=1, validation_data=(x_test, y_test))\n",
    "model.save(\"fire_detection.model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the pre-trained fire detection model\n",
    "model = load_model(\"fire_detection.model.h5\")\n",
    "\n",
    "def detect_fire(frame, threshold=0.5):\n",
    "    # Resize and preprocess the frame (assuming model expects grayscale 48x48 input)\n",
    "    preprocess_frame = cv2.cvtColor(cv2.resize(frame, (48, 48)), cv2.COLOR_BGR2GRAY)\n",
    "    preprocess_frame = np.expand_dims(preprocess_frame, axis=0)\n",
    "    preprocess_frame = np.expand_dims(preprocess_frame, axis=-1)\n",
    "    preprocess_frame = preprocess_frame.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Make prediction using the model\n",
    "    prediction = model.predict(preprocess_frame)\n",
    "\n",
    "    # Check if fire is detected based on the model's output and the threshold\n",
    "    if prediction[0][1] >= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\AMZ PC\\OneDrive\\Desktop\\CV\\Smoke detection\\3826855-hd_1920_1080_24fps.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: could not open video file\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Check if fire is detected in the frame\n",
    "    if detect_fire(frame):\n",
    "        # Draw a rectangle and display a warning message\n",
    "        cv2.rectangle(frame, (100, 100), (frame.shape[1] - 100, frame.shape[0] - 100), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Warning, fire detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the processed video\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
